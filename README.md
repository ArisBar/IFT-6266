# IFT-6266 Project

## Conditional Image Generation

### In a nutshell

**Goal**. The project is to generate the center region of images conditioned on the outside border and a caption. 
The task is applied on images from the mscoco dataset downsampled to 64 x 64 images, where the 32 x 32 center is masked out:

**Approach amd results**. I focussed on conditioning on the outside border, leaving the inclusion of captions for future work. 
I approached the problem using deep convolutional autoencoder architectures akin to Ref [1], with L2 and adversarial losses [2]. 
I obtained my best results with a relatively simple fully convolutional architecture with skip connections, and a weighted combination of a L2  loss and a Wasserstein GAN (WGAN) loss [3].


Details of my experiments are given in the next sections.

**Implementation**.
I build my models by modifying the original DCGAN code [4] in core Theano. Note that the code uses the old cuda Theano backend.  


### Baseline: Autoencoder with L2 loss 
My baseline model for this project is an autoencoder (AE) with convolution and deconvolution layers and L2 loss reconstruction of the center.  The masked 64 x 64 image are  downsampled all the way down to 1 x 1;  and back to a 32 x 32 center for the image.  My deconv layers here simply consists in an upsampling followed with a standard convolution. I've used a L2 loss reconstruction for the centre.  

I've experimented with variants of the following architecture:

| Layer | Input | Output | Kernel size |                 
| ------|-------|--------|-------------|
| conv1 | 3 x 64 x 64 | 64 x 32 x 32 | 3 x 3 |
| conv2 | 64 x 32 x 32 | 128 x 16 x 16 | 3 x 3 |
| conv3 |  128 x 16 x 16 | 256 x 8 x 8 | 3 x 3 |
| conv4 |  256 x 8 x 8 | 512 x 4 x 4 | 3 x 3 |
| conv4 | 512 x 4 x 4 | 512 x 1 x 1 | 4 x 4
| deconv1 | 512 x 1 x 1 | 256 x 4 x 4 | 3 x 3 |
| deconv2 |  256 x 4 x 4 | 128 x 8 x 8 | 3 x 3 |
| deconv3 | 128  x 8 x 8 | 64 x 4 x 4 | 3 x 3 |
| deconv4 | 128 x 16 x 16 | 3 x 32 x 32 | 3 x 3 |

Examples of images generated by this baseline are as follows: 

#### Autoencoder with L2 loss: Training (left) and validation (right) images                

![AE_training](/images/train195.png)   !![AE_validation](/images/val195.png) 

Although the performance of this baseline is not striking, we see that to some extent hthe model is already able to extend edges, texture and colors from the border. 

I've experimented with variants including a channel-wise dense layer as in Ref [2] and a fully dense layer, without noticing significant improvement. Later in the context of GAN I would choose to lighten a bit this architecture by removing conv4 and deconv1, reduce the nnumer fo filters (max 256) and work with larger kernel size (5x5). I believe these modifications should also improve the performance of the AE.  

### GAN architectures 

### Reinforced context 

### Outlook

### Acknowledgments. 

### References

[1] Context Encoders: Feature Learning by Inpainting.  https://arxiv.org/abs/1604.07379

[2] Generative adversarial Networks. https://arxiv.org/abs/1406.2661

[3] Wasserstein GAN.  https://arxiv.org/abs/1701.07875.

[4] Deep Convolutional Generative Adversarial Networks. Code available as  https://github.com/Newmu/dcgan_code



